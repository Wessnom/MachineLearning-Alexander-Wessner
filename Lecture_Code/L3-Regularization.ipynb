{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kokchun/Machine-learning-AI22/blob/main/Lecture_code/L4-Regularization.ipynb\" target=\"_parent\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; for interacting with the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture notes - Regularized linear models\n",
    "---\n",
    "\n",
    "This is the lecture note for **regularized linear models**\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that this lecture note gives a brief introduction to regularized linear models. I encourage you to read further about regularized linear models. </p>\n",
    "\n",
    "Read more:\n",
    "\n",
    "- [Regularized linear models medium](https://medium.com/analytics-vidhya/regularized-linear-models-in-machine-learning-d2a01a26a46)\n",
    "- [Ridge regression wikipedia](https://en.wikipedia.org/wiki/Ridge_regression)\n",
    "- [Tikhonov regularization wikipedia](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
    "- [Lasso regression wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics))\n",
    "- [Korsvalidering](https://sv.wikipedia.org/wiki/Korsvalidering)\n",
    "- [Cross validation](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
    "- [Scoring parameter sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [ISLP pp 240-253](https://www.statlearning.com/)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4589/3025487750.py:5: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-white\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134, 19), (66, 19), (134,), (66,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "df = pd.read_csv(\"../data/Advertising.csv\", index_col=0)\n",
    "X, y = df.drop(\"Sales\", axis = 1), df[\"Sales\"]\n",
    "# detta är en multipel polynomiell regression\n",
    "# termerna innehåller x1³, x2³, x3³, x1³x2³x3³, x1², x2², x3², x1²x2²x3² ..., x1x2, x1x3 ... osv. Blir 19 features!\n",
    "# övning 3 (e02) har en uppgift för att hitta graden nedan. Denna är vald konservativt, eftersom antalet features snabbt ökar\n",
    "# med graden för polynomet och vi vill undvika overfit\n",
    "model_polynomial = PolynomialFeatures(3, include_bias=False)\n",
    "poly_features = model_polynomial.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature standardization\n",
    "Remove sample mean and divide by sample standard deviation \n",
    "\n",
    "$X' = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "LASSO, Ridge and Elasticnet regression that we'll use later require that the data is scaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled X_train mean -0.00, std 1.00\n",
      "Scaled X_test mean -0.12, std 1.12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled X_train mean {scaled_X_train.mean():.2f}, std {scaled_X_train.std():.2f}\")\n",
    "print(f\"Scaled X_test mean {scaled_X_test.mean():.2f}, std {scaled_X_test.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Regularization techniques\n",
    "\n",
    "Problem with overfitting was discussed in previous lecture. When model is too complex, data noisy and dataset is too small the model picks up patterns in the noise. The output of a linear regression is the weighted sum: \n",
    "$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\ldots + \\theta_nx_n$ , where the weights $\\theta_i$ represents the importance of the $ith$ feature. Want to constrain the weights associated with noise, through regularization. We do this by adding a regularization term to the cost function used in training the model. Note that the cost function for evaluation now will differ from the training.\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> most regularization model requires scaling of data </p>\n",
    "\n",
    "---\n",
    "### Ridge regression \n",
    "Also called Tikhonov regularization or $\\ell_2$ regularization.\n",
    "\n",
    "$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda \\frac{1}{2}\\sum_{i=1}^n \\theta_i^2$\n",
    "\n",
    "where $\\lambda \\ge 0$ is the ridge parameter or the penalty term, which reduces variance by increasing bias. Observe that the sum starts from 1, so the bias term $\\theta_0$ is not affected by $\\lambda$. Therefore by the larger the $\\lambda$ the more $\\theta_i, i = {1,2,\\ldots}$ causes higher error. As variance is decreasing and bias increasing, the model fits worse to the training datas noise and generalizes better.\n",
    "\n",
    "From the closed form OLS solution to ridge regression, we see that $\\lambda = 0$ gives us the normal equation for linear regression: \n",
    "\n",
    "$\\hat{\\vec{\\theta}} = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6109310380380405, 0.484595999454497)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def ridge_regression(X, penalty=0):\n",
    "    # alpha = 0 should give linear regression\n",
    "    # note that alhpa is same as lambda in theory, i.e. penalty term. sklearn has chosen alpha to generalize their API\n",
    "    model_ridge = Ridge(alpha=penalty) \n",
    "    model_ridge.fit(scaled_X_train, y_train)\n",
    "    y_pred = model_ridge.predict(X)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "y_pred = ridge_regression(scaled_X_test, 0.2)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "RMSE, mean_absolute_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5148267621786607, 0.3748516441217833)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with linear regression -> RMSE very similar!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(scaled_X_train, y_train)\n",
    "y_pred = model_linear.predict(scaled_X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.89480144  0.42062367  0.         -0.          3.55216501  0.\n",
      "  0.          0.01110965  0.         -0.42677394 -0.         -0.\n",
      "  0.          0.         -0.          0.          0.06706906  0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model_lasso = Lasso(alpha = .1)\n",
    "model_lasso.fit(scaled_X_train, y_train)\n",
    "y_pred = model_lasso.predict(scaled_X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)\n",
    "print(model_lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross-validation\n",
    "\n",
    "One strategy to choose the best hyperparameter alpha is to take the training part of the data and \n",
    "1. shuffle dataset randomly\n",
    "2. split into k groups\n",
    "3. for each group -> take one test, the rest training -> fit the model -> predict on test -> get evaluation metric\n",
    "4. take the mean of the evaluation metrics\n",
    "5. choose the parameters and train on the entire training dataset\n",
    "\n",
    "Repeat this process for each alpha, to see which yielded lowest RMSE. k-fold cross-validation: \n",
    "- good for smaller datasets\n",
    "- fair evaluation, as a mean of the evaluation metric for all k groups is calculated\n",
    "- expensive to compute as it requires k+1 times of training\n",
    "\n",
    "---\n",
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV # ridge regression with cross-validation\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "#print(SCORERS.keys())\n",
    "# negative because sklearn uses convention of higher return values are better\n",
    "model_ridgeCV = RidgeCV(alphas = [.0001, .001, .01, .1, .5, 1, 5, 10], scoring = \"neg_mean_squared_error\")\n",
    "model_ridgeCV.fit(scaled_X_train, y_train)\n",
    "print(model_ridgeCV.alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5635899169556714, 0.4343075766486241)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best alpha is 0.1\n",
    "# it seems that linear regression outperformed ridge regression in this case\n",
    "# however it could depend on the distribution of the train|test data, so using alpha = 0.1 is more robust here\n",
    "y_pred = model_ridgeCV.predict(scaled_X_test)\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "RMSE, mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.84681185,  0.52142086,  0.71689997, -6.17948738,  3.75034058,\n",
       "       -1.36283352, -0.08571128,  0.08322815, -0.34893776,  2.16952446,\n",
       "       -0.47840838,  0.68527348,  0.63080799, -0.5950065 ,  0.61661989,\n",
       "       -0.31335495,  0.36499629,  0.03328145, -0.13652471])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridgeCV.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.004968802520343366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5785146895301977, 0.46291883026932984)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# it is trying 100 different alphas along regularization path epsilon\n",
    "model_lassoCV = LassoCV(eps = 0.001, n_alphas = 100, max_iter=10000, cv=5)\n",
    "model_lassoCV.fit(scaled_X_train, y_train)\n",
    "print(f\"alpha = {model_lassoCV.alpha_}\")\n",
    "\n",
    "y_pred = model_lassoCV.predict(scaled_X_test)\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.19612354,  0.43037087,  0.29876351, -4.80417579,  3.46665205,\n",
       "       -0.40507212,  0.        ,  0.        ,  0.        ,  1.35260206,\n",
       "       -0.        ,  0.        ,  0.14879719, -0.        ,  0.        ,\n",
       "        0.        ,  0.09649665,  0.        ,  0.04353956])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we notice that many coefficients have been set to 0 using Lasso\n",
    "# it has selected some features for us \n",
    "model_lassoCV.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net\n",
    "\n",
    "Elastic net is a combination of both Ridge l2-regularization and Lasso l1-regularization. The cost function to be minimized for elastic net is: \n",
    "\n",
    "$$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda\\left(\\alpha\\sum_{i=1}^n |\\theta_i| + \\frac{1-\\alpha}{2}\\sum_{i=1}^n \\theta_i^2\\right)$$\n",
    "\n",
    ", where $\\alpha$ here determines the ratio for $\\ell_1$ or $\\ell_2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 ratio: 1.0\n",
      "alpha 0.004968802520343366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# note that alpha here is lambda in the theory\n",
    "# l1_ratio is alpha in the theory\n",
    "model_elastic = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], eps = 0.001, n_alphas = 100, max_iter=10000)\n",
    "model_elastic.fit(scaled_X_train, y_train)\n",
    "print(f\"L1 ratio: {model_elastic.l1_ratio_}\") # this would remove ridge and pick Lasso regression entirely\n",
    "print(f\"alpha {model_elastic.alpha_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5785146895301977, 0.46291883026932984)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_elastic.predict(scaled_X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)\n",
    "# note that the result is same for Lasso regression which is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Kokchun Giang\n",
    "\n",
    "[LinkedIn][linkedIn_kokchun]\n",
    "\n",
    "[GitHub portfolio][github_portfolio]\n",
    "\n",
    "[linkedIn_kokchun]: https://www.linkedin.com/in/kokchungiang/\n",
    "[github_portfolio]: https://github.com/kokchun/Portfolio-Kokchun-Giang\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99284c818dbeb598aa1f68004bbb0b5edd120a68b4d8720a3a3dd099e220abe1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('ML_exploration-5IpxFwVQ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
